# -*- coding: utf-8 -*-
"""Final Data_science_employee_salary_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q5pwnEI_eVLOEQOp9oehJCyVK--6KYmN
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
sns.set_theme(color_codes=True)

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('ds_salaries (1).csv')

"""# DATA PREPROCESSING PART 1"""

df.drop(columns=['salary','salary_currency'],inplace=True)               #drop salary and salary column and make usd universal
df.head()

check_missing=df.isnull().sum()*100/df.shape[0]                         #check the missing value
check_missing[check_missing>0].sort_values(ascending=False)

df.select_dtypes(include='object').nunique()       #check the number of unique value in an object datatype

"""# CATEGORIZE THE JOB TITLE"""

df.job_title.unique()

def segment_job_title(job_title):
    Data_scientist_titles=['Principal Data Scientist','Data Scientist','Applied Scientist','Research Scientist','Applied Data Scientist']
    Machine_learning_titles=['ML Engineer','Machine Learning Engineer','Applied Machine Learning Engineer','Machine Learning Software Engineer','NLP Engineer']
    Data_analyst_titles=['Data Analyst', 'Data Quality Analyst','Compliance Data Analyst', 'Business Data Analyst','Data Analytics Manager']
    Data_engineer_titles=['Data Modeler', 'Data Engineer','ETL Engineer','Data DevOps Engineer','Data Science Engineer','Data Infrastructure Engineer']
    Bi_analytics_titles=['Data Analytics Manager','Computer Vision Engineer','AI Developer','Big Data Architect''Head of Data Science']
    Other_titles=['BI Data Engineer','Director of Data Science', 'Machine Learning Scientist','MLOps Engineer', 'AI Scientist', 'Autonomous Vehicle Technician','Applied Machine Learning Scientist', 'Lead Data Scientist','Cloud Database Engineer', 'Financial Data Analyst','Data Infrastructure Engineer', 'Software Data Engineer','AI Programmer', 'Data Operations Engineer', 'BI Developer','Data Science Lead', 'Deep Learning Researcher', 'BI Analyst','Data Science Consultant', 'Data Analytics Specialist']

    if job_title in Data_scientist_titles:
        return 'Data scientist'
    elif job_title in Machine_learning_titles:
        return 'Machine Learning Engineer'
    elif job_title in Data_analyst_titles:
        return 'Data Analyst'
    elif job_title in Data_engineer_titles:
        return 'Data Engineer'
    elif job_title in Bi_analytics_titles:
        return 'Business Intelligence and Analytics'
    elif job_title in Other_titles:
        return 'Other'
    else:
        return 'Uncategorized'

df['job_title']=df['job_title'].apply(segment_job_title)

plt.figure(figsize=(10,5))
df['job_title'].value_counts().plot(kind='bar')

"""# CATEGORIZE THE EMPLOYEE RESIDENCE"""

df.employee_residence.unique()

def categorize_region(country):
    if country in ['DE','GB','PT','NL','CH','CF','FR','FI','UA','IE','AT']:
        return 'Europe'
    elif country in ['US','CA','MX']:
        return 'North America'
    elif country in ['BR','AR','CL','BO','CR','DO','PR','HN','UV']:
        return 'South America'
    elif country in ['NG','GH','KE','TN','DZ']:
        return 'Africa'
    elif country in ['HK','IN','CN','JP','KR','BD','VN','PH','MY','ID','AE']:
        return 'Asia'
    elif country in ['AU','NZ']:
        return 'Oceania'
    else :
        return 'Unknown'

df['employee_residence']=df['employee_residence'].apply(categorize_region)

plt.figure(figsize=(10,5))
df['employee_residence'].value_counts().plot(kind='bar')

"""# CATEGORIZE THE COMPANY LOCATION"""

df.company_location.unique()

df['company_location']=df['company_location'].apply(categorize_region)

plt.figure(figsize=(10,5))
df['company_location'].value_counts().plot(kind='bar')

#check the number of unique values on object datatype
df.select_dtypes(include='object').nunique()

"""# EXPLORATORY DATA  ANALYSIS

"""

df.remote_ratio.unique()

#list of categorical variables to plot
cat_vars=['experience_level','employment_type','job_title','employee_residence',
          'company_location','company_size','remote_ratio']

#create figure with subplots
fig,axs=plt.subplots(nrows=2,ncols=4,figsize=(20,10))
axs=axs.flatten()

#create barplot for each categroical variable
for i,var in enumerate(cat_vars):
    sns.barplot(x=var,y='salary_in_usd',data=df,ax=axs[i],estimator=np.mean)
    axs[i].set_xticklabels(axs[i].get_xticklabels(),rotation=90)

#remove the eighth subplot
fig.delaxes(axs[7])

#adjust spacing between subplots
fig.tight_layout()

#show plot
plt.show()

sns.set_style("darkgrid")

sns.set_palette("Set2")

sns.lineplot(x='work_year',y='salary_in_usd',hue='job_title',data=df,errorbar=None,estimator=np.mean)

plt.title("salary in USD by work year and job title")

plt.xlabel("work year")

plt.ylabel("salary in USD")

plt.show

"""# DATA PREPROCESSING PART 2
# LABEL ENCODING FOR OBJECT DATATYPE
"""

#LOOP FOR EACH COLUMN IN THE DATAFRAME WHERE DTYPE IS OBJECT
for col in df.select_dtypes(include=['object']).columns:

    #print the column name and unique values
    print(f"{col}:{df[col].unique()}")

from sklearn import preprocessing

#loop over each column in the dataframe where dtype is object
for col in df.select_dtypes(include=['object']).columns:

    #initialize a label encoder object
    label_encoder=preprocessing.LabelEncoder()

    #fit the encoder to the unique values in column
    label_encoder.fit(df[col].unique())

    #transform the column using encoder
    df[col]=label_encoder.transform(df[col])

    #print the column name and the unique encoded value
    print(f"{col}:{df[col].unique()}")

df.dtypes

"""# ALL OF THE DATA ARE CATEGORICAL SO IT MEANS NO OUTLIERS

# TRAIN TEST SPLIT
"""

x=df.drop('salary_in_usd',axis=1)
y=df['salary_in_usd']

#test size 20% and train size 80%
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)

""" DECISION TREE REGRESSOR"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV
#from sklearn.datasets import load_boston
import pandas as pd
import numpy as np

data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

#create a DecesionTreeRegressor object
dtree=DecisionTreeRegressor()

#define the hyperparameters to tune and their values
param_grid={
    'max_depth':[2,4,6,8],
    'min_samples_split':[2,4,6,8],
    'min_samples_leaf':[1,2,3,4],
    'max_features':[1.0,'sqrt','log2']
}

#create a gridsearchCV object
grid_search=GridSearchCV(dtree,param_grid,cv=5,error_score='raise')

#fit the grid SearchCV object to the data
grid_search.fit(x_train,y_train)

#print the best hyperparameters
print(grid_search.best_params_)

from sklearn.tree import DecisionTreeRegressor
dtree=DecisionTreeRegressor(random_state=0,max_depth=6,max_features=1.0,min_samples_leaf=3,
                          min_samples_split=4)
dtree.fit(x_train,y_train)

from sklearn import metrics
from sklearn.metrics import mean_absolute_percentage_error
import math
y_pred=dtree.predict(x_test)
mae=metrics.mean_absolute_error(y_test,y_pred)
mape=mean_absolute_percentage_error(y_test,y_pred)
mse=metrics.mean_squared_error(y_test,y_pred)
r2=metrics.r2_score(y_test,y_pred)
rmse=math.sqrt(mse)

print('MAE is {}'.format(mae))
print('MAPE is {}'.format(mape))
print('MSE is {}'.format(mse))
print('R2 is {}'.format(r2))
print('RMSE is {}'.format(rmse))

imp_df=pd.DataFrame({
    "Feature Name":x_train.columns,
    "Importance":dtree.feature_importances_
})
fi=imp_df.sort_values(by="Importance",ascending=False)
fi2=fi.head(10)
plt.figure(figsize=(10,8))
sns.barplot(data=fi2,x='Importance',y='Feature Name')
plt.title('Feature Importance each attributes (decesion tree regressor)',fontsize=18)
plt.xlabel('Importance',fontsize=16)
plt.ylabel('Feature  Name',fontsize=16)
plt.show()

"""# RANDOM FOREST REGRESSOR"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV


#create a RandomForestRegressor object
rf=RandomForestRegressor()

#define the hyperparameters grid
param_grid={
    'max_depth':[3,5,7,9],
    'min_samples_split':[2,5,10],
    'min_samples_leaf':[1,2,4]

}

#create a gridsearchCV object
grid_search=GridSearchCV(rf,param_grid,cv=5,scoring='r2')

#fit the grid SearchCV object to the training data
grid_search.fit(x_train,y_train)

#print the best hyperparameters
print("Best Hyper parameters:",grid_search.best_params_)

from sklearn.ensemble import RandomForestRegressor
rf=RandomForestRegressor(random_state=0,max_depth=7,min_samples_split=10,min_samples_leaf=2)
rf.fit(x_train,y_train)

from sklearn import metrics
from sklearn.metrics import mean_absolute_percentage_error
import math
y_pred=rf.predict(x_test)
mae=metrics.mean_absolute_percentage_error(y_test,y_pred)
mape=mean_absolute_percentage_error(y_test,y_pred)
mse=metrics.mean_squared_error(y_test,y_pred)
r2=metrics.r2_score(y_test,y_pred)
rmse=math.sqrt(mse)

print('MAE is{}'.format(mae))
print('MAPE is {}'.format(mape))
print('MSE is {}'.format(mse))
print('R2  score is {}'.format(r2))
print('RMSE score is {}'.format(rmse))

imp_df=pd.DataFrame({
    "Feature Name":x_train.columns,
    "Importance":dtree.feature_importances_
})
fi=imp_df.sort_values(by="Importance",ascending=False)
fi2=fi.head(10)
plt.figure(figsize=(10,8))
sns.barplot(data=fi2,x='Importance',y='Feature Name')
plt.title('Feature Importance each attributes (Random Forest regressor)',fontsize=18)
plt.xlabel('Importance',fontsize=16)
plt.ylabel('Feature  Name',fontsize=16)
plt.show()

from sklearn.linear_model import LinearRegression

# Create a LinearRegression object
linear_reg = LinearRegression()

# Fit the Linear Regression model to your data
linear_reg.fit(x_train, y_train)

# Make predictions with the linear regression model
y_pred = linear_reg.predict(x_test)
mae=metrics.mean_absolute_percentage_error(y_test,y_pred)
mape=mean_absolute_percentage_error(y_test,y_pred)
mse=metrics.mean_squared_error(y_test,y_pred)
r2=metrics.r2_score(y_test,y_pred)
rmse=math.sqrt(mse)

print('MAE is{}'.format(mae))
print('MAPE is {}'.format(mape))
print('MSE is {}'.format(mse))
print('R2  score is {}'.format(r2))
print('RMSE score is {}'.format(rmse))

model=LinearRegression()
linear_reg.fit(x_train, y_train)

imp_df=pd.DataFrame({
    "Feature Name":x_train.columns,
    "Importance":dtree.feature_importances_
})
fi=imp_df.sort_values(by="Importance",ascending=False)
fi2=fi.head(10)
plt.figure(figsize=(10,8))
sns.barplot(data=fi2,x='Importance',y='Feature Name')
plt.title('Feature Importance each attributes (LinearRegression)',fontsize=18)
plt.xlabel('Importance',fontsize=16)
plt.ylabel('Feature  Name',fontsize=16)
plt.show()